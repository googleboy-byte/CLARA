{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf0dbaad-b5d3-46f0-8191-0beb7f064d05",
   "metadata": {},
   "source": [
    "# I. Dataset & Input Design\n",
    "\n",
    "### Question 1: Why were only SPOC 2-minute cadence light curves used?\n",
    "\n",
    "We used 2-minute cadence SPOC light curves because they provide a manageable dataset size (~15k–20k light curves per sector) with high-quality, preprocessed data and confirmed TOIs. This makes them ideal for experimentation on a modest system (32GB RAM, 4-core i3 CPU), and provides a fair testbed without overwhelming computational resources.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-up 1.1: Why not use more stars or the full million-star dataset?\n",
    "\n",
    "More stars were not required to test our hypothesis. We selected a representative subset with a fair distribution of TOIs, sufficient to evaluate model behavior under resource constraints.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-up 1.2: Why not use FFI (30-minute cadence) light curves?\n",
    "\n",
    "FFI light curves are extremely large in volume, making them impractical for this phase of the project on our available hardware. Our goal was not exhaustive coverage, but controlled hypothesis testing.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-up 1.3: Why did you move from Sector 1 to Sector 2?\n",
    "\n",
    "The first four URF models were trained and tested on Sector 1. However, URF-3, trained using synthetic features generated from TOIs, overfitted to Sector 1 data. To confirm this, we tested it on Sector 2, which revealed generalization failure. Due to space limitations, only one sector could be held at a time. Sector 2 was retained for follow-up experiments and URF-4 development. We plan to revisit cross-sector validation in CLARA Part 3.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-up 1.4: Why were TOIs used at all in an unsupervised setting?\n",
    "\n",
    "Since we work with unlabeled data, we needed a ground-truth proxy to benchmark performance. TOIs represent scientifically validated transit signals and serve as a useful reference to evaluate anomaly detection quality.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-up 1.5: Were TOIs used for training?\n",
    "\n",
    "Yes, in URF-3. The synthetic set for URF-3 was constructed from features derived from known TOIs. However, the model overfit to Sector 1 and failed to generalize to Sector 2. This led to the development of URF-4, which relies on synthetic light curves with user-controlled features.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-up 1.6: Why use synthetic light curves (e.g., with `batman`) in URF-4?\n",
    "\n",
    "URF-4 was built to test synthetic set design systematically. Using tools like `batman`, we can generate synthetic LCs with controlled input parameters — such as transit count, duration, cadence, and noise — making it possible to study how these factors influence URF model behavior.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-up 1.7: Why not use TOIs for synthetic set design?\n",
    "\n",
    "While TOIs are useful for evaluation, they lack standardized, controllable input parameters. Synthetic curves, on the other hand, allow precise tuning of design variables like duration, cadence, and noise. This makes them ideal for constructing interpretable and tunable synthetic training sets for URF models.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2: Could the results change if we use FFI-based light curves?\n",
    "\n",
    "Yes, the results would likely change if FFI-based light curves were used. FFI (Full Frame Image) light curves are not subject to the same target pre-selection as 2-minute cadence light curves, and they represent a broader, less filtered stellar population. As a result, the definition of \"normal\" behavior in light curves would shift, potentially altering the anomaly scoring and the mapping between synthetic set parameters and model performance. While our hypothesis — that synthetic set design can steer model behavior — is expected to hold, the specific mappings of input features (e.g., number of transits, duration) to performance metrics (e.g., TOI recall) would likely differ.\n",
    "\n",
    "\n",
    "### Follow-Up 2.1: Why would FFI-based light curves change the anomaly landscape?\n",
    "\n",
    "2-minute cadence light curves are drawn from pre-selected targets, typically nearby dwarf stars, and tend to exhibit more stable, well-characterized variability. In contrast, FFI data includes a much wider range of stellar types and behaviors, including less stable or more variable stars. This broader variability spectrum would redefine what is considered \"normal,\" leading the URF to assign different anomaly scores. Consequently, the anomaly score thresholds — currently calibrated using TOIs from 2-min data — would not directly apply to FFI data.\n",
    "\n",
    "\n",
    "### Follow-Up 2.2: Would anomaly score thresholds need to change for FFI-based models?\n",
    "\n",
    "Yes. Since FFI data would establish a different baseline for what is \"normal,\" the anomaly score distribution would shift. Thresholds based on 2-min cadence TOIs would no longer be valid for performance evaluation. A new evaluation strategy would be needed, possibly using synthetic injections or another benchmark tailored to the FFI population.\n",
    "\n",
    "\n",
    "### Follow-Up 2.3: If TOIs can't serve as benchmarks for FFI-based URFs, how should model performance be evaluated?\n",
    "\n",
    "When TOIs are insufficient or non-representative (as in FFI studies), we can use synthetic light curves with known injected features (e.g., transits, flares) as a benchmarking strategy. This allows for controlled testing of model sensitivity and precision by comparing detection outcomes against known injected events.\n",
    "\n",
    "\n",
    "### Follow-Up 2.4: Can FFI-based models find anomalies that are missed in 2-min cadence data?\n",
    "\n",
    "This has not yet been tested. However, if the FFI-based population defines a new normal, then some light curves that were considered normal in 2-min cadence analysis might appear anomalous when compared against FFI variability patterns. Once Part 2 clustering is complete, we can examine whether new anomaly types emerge uniquely in FFI-trained models.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3: How representative are the 16k light curves per sector?\n",
    "  \n",
    "The ~16,000 light curves per sector (from SPOC 2-minute cadence targets) may not be fully representative of the entire TESS dataset, particularly compared to FFI-based light curves. However, they are standardized, high-quality, and computationally manageable — making them suitable for hypothesis testing. Their use is also consistent with prior work, such as Crake & Martínez-Galarza (2023), which this study builds upon.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-Up 3.1: In what sense are these 16k curves possibly *not* representative of full TESS sectors?\n",
    "\n",
    "\n",
    "They exclude the vast majority of stars observed in TESS sectors — especially those only available through FFIs. The 2-min targets are typically bright, nearby dwarfs selected for known scientific interest, leading to a biased sample with less astrophysical diversity.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-Up 3.2: How did you account for any imbalance or bias when selecting test subsets from the 16k curves?\n",
    "\n",
    "\n",
    "We used stratified random sampling to match the TOI-to-total curve ratio of the full 16k set. This method was used both for the 4000-light curve test subset (used in URF-4 subvariant evaluation) and the 10 subsets (used in α-variant testing). It ensured uniformity and fair comparison across models.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-Up 3.3: Why is this sample still valid for hypothesis testing?\n",
    "\n",
    "\n",
    "Because the primary goal was to test how synthetic set design affects URF performance in a controlled setting. The relative behavior of models — not the absolute astrophysical coverage — was the focus. This dataset offers clarity, consistency, and efficiency for such controlled evaluations.\n",
    "\n",
    "\n",
    "### Follow-Up 3.4: What limitations does this dataset impose on the generalizability of your conclusions?\n",
    "\n",
    "\n",
    "Being restricted to 2-min cadence targets means the results might not generalize to FFI data, which includes more diverse and noisier light curves. Definitions of normality, anomaly thresholds, and performance metrics like TOI recall or importance AUC may shift significantly with FFI-based studies.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-Up 3.5: How could future CLARA research address the representativeness limitations of the current 16k light curve subset?\n",
    "\n",
    "\n",
    "By scaling testing to FFI-based datasets using high-performance computing. This would allow us to evaluate how the performance-to-input feature mapping changes across a broader population and define correction strategies or new models suited for FFI-normalized anomaly detection.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Question 4: Are TOIs evenly distributed in the set? Are there biases?\n",
    "\n",
    "\n",
    "No, TOIs are not evenly distributed. In Sector 1, there were 175 TOI-associated FITS files out of ~16,000 light curves; in Sector 2, the number was 195. The distribution appears sparse, and there is no evidence of uniform coverage across all TICs in a sector.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-Up 4.1 Have you tried to quantify TOI biases with respect to sky position or magnitude?  \n",
    "No. We have not yet correlated astrometric or astrophysical properties to the TESS data. For URF feature extraction, we used only flux time series and Lomb-Scargle periodogram power values (vector sizes 3000 and 1000 respectively).\n",
    "\n",
    "\n",
    "\n",
    "### Follow-Up 4.2 Would this bias in TOI distribution affect the model’s ability to generalize?  \n",
    "This has not been definitively proven, but we believe not. The bias is consistent (~175–195 TOIs out of 16k light curves per sector). Since only 1500 curves were used as the real feature set during URF training and hyperparameter search, and the URF-4 α = 0.5 model passed the generalization test, the model appears robust to such distributional bias.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-Up 4.3 Do we know what selection effects determine which TICs are assigned TOIs in SPOC data?  \n",
    "No. This hasn't been explored in our work. TOIs are used solely as a proxy benchmark for evaluating anomaly detection, not as an unbiased sample of transit candidates.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-Up 4.4 Do TOIs cluster in anomaly score space, or are they evenly spread?  \n",
    "They tend to cluster near the top of the anomaly score distribution. In our analysis, high-importance TOIs were often found within the top 5–20% of the anomaly score percentiles, especially in balanced and low-α URF-4 variants.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-Up 4.5 Could uneven representation of astrophysical TOI types affect model performance across anomaly categories?  \n",
    "Possibly. Our synthetic sets used fixed parameter configurations and transit models (box or Mandel-Agol). If TOIs skew heavily toward certain types (e.g., deep, short-period hot Jupiters), then less-represented types may be underperforming. However, generalization tests suggest the model handles diverse anomaly shapes reasonably well.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 5: What preprocessing (detrending, smoothing, normalization) was used, and how does it affect anomalies?\n",
    "\n",
    "For each light curve, we generated a 4000-length feature vector:  \n",
    "- The first 3000 values were normalized PDCSAP flux points from the start of the light curve.  \n",
    "- The remaining 1000 values came from the Lomb-Scargle periodogram’s power spectrum.  \n",
    "\n",
    "This setup captures both the direct flux time series and its dominant periodic features — enabling the URF to learn from both temporal behavior and frequency domain patterns. Normalization was critical to ensure comparability between flux values across stars, and truncation was used for computational efficiency.\n",
    "\n",
    "\n",
    "### Follow-Up 5.1: Why 3000 flux points and 1000 power values specifically?\n",
    "  \n",
    "The decision was inspired by Crake & Martínez-Galarza (2023), who also used flux + periodogram features for anomaly detection.  \n",
    "Practically, using all ~19,000 flux points is computationally expensive. We assumed that if any periodic anomaly exists, its signature would appear in the first 3000 points.  \n",
    "This trade-off balances detection capability and computational tractability.\n",
    "\n",
    "\n",
    "### Follow-up 5.2: How was normalization applied, and why is it important?\n",
    " \n",
    "Normalization was done using `.normalize()` from `astropy`, applied after creating the light curve from time and flux data.  \n",
    "It standardizes the scale of flux values across all stars, allowing the URF to compare light curves meaningfully.  \n",
    "Without normalization, variations due to stellar brightness differences could obscure true anomalies.\n",
    "\n",
    "\n",
    "### Follow-up 5.3: Why was truncation to 3000 flux points considered safe?\n",
    "  \n",
    "While full light curves have ~19,000 points, analyzing all is inefficient and often redundant.  \n",
    "Since many periodic anomalies (e.g. transits or flares) repeat or occur early, we assumed the first 3000 points would usually capture relevant behavior.  \n",
    "This assumption held well during URF training and testing phases.\n",
    "\n",
    "\n",
    "### Follow-up 5.4: Why was the Lomb-Scargle periodogram used, and how do its power values help in detecting anomalies?\n",
    "  \n",
    "Lomb-Scargle is ideal for unevenly sampled time series like TESS light curves.  \n",
    "Its power spectrum identifies dominant periodicities, which helps the URF learn what constitutes a \"periodic anomaly.\"  \n",
    "The peak frequency from the power spectrum is later used to phase-fold light curves and study recurring patterns more clearly.\n",
    "\n",
    "\n",
    "### Follow-up 5.5: How might preprocessing choices introduce biases or affect anomaly detection?\n",
    "  \n",
    "Normalization and truncation can inadvertently suppress low-amplitude or long-duration anomalies.  \n",
    "For instance, an overly aggressive normalization may flatten subtle dips, and truncating at 3000 points may miss late-occurring periodic events.  \n",
    "Thus, preprocessing decisions involve trade-offs between speed, sensitivity, and signal completeness.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefa6439-b42e-448e-801e-4a985fcc2ce5",
   "metadata": {},
   "source": [
    "# II. URF Architecture & Model Logic\n",
    "\n",
    "### Question 1: Why URF instead of other anomaly detectors (e.g., autoencoders, isolation forests, VAEs)?\n",
    "\n",
    "Crake and Martínez-Galarza (2023) used unsupervised random forests (URFs) for anomaly detection. Our project began as a replication of that approach, aiming to identify new anomalies using URFs. However, we later discovered that URFs could be tuned to exhibit specific behaviors based on how their synthetic contrast set was designed. This prompted us to explore the broader behavior space of URFs by varying synthetic set parameters and analyzing performance metrics such as TOI recall and feature importance.\n",
    "\n",
    "### Follow-up 1.1: Were other models (autoencoders, VAEs, isolation forests) tested?\n",
    "No. This study focused exclusively on URFs to extend the methodology of Crake & MG (2023). While other anomaly detection models could be valuable, our aim was to systematically understand the behavior of URFs under controlled synthetic set designs.\n",
    "\n",
    "### Follow-up 1.2: Can URFs be controlled to favor different scientific goals?\n",
    "Yes. From our experiments, we observed that different synthetic set configurations lead to URFs with distinct performance profiles. We defined two key metrics — TOI recall and TOI importance — and demonstrated that URFs could be tuned to favor either by adjusting the input features of the synthetic set. These preferences remained consistent across sectors, demonstrating generalizable model behavior.\n",
    "\n",
    "### Follow-up 1.3: Are there limitations to what URFs can learn?\n",
    "Yes. URFs exhibit predictable behavior only within specific ranges of synthetic set design parameters. For example, noise levels between 50 and 300 ppm consistently produced viable models, while values outside this range led to erratic or degenerate behavior. Additionally, conflicting goals like maximizing recall and importance while minimizing anomaly rate could not always be satisfied together.\n",
    "\n",
    "### Follow-up 1.4: Why is URF considered scientifically interpretable?\n",
    "URFs allow researchers to define and prioritize their goals through interpretable metric combinations. By using a small labelled set of 4000 light curves with known TOIs, we computed a correlation matrix between metrics and applied a combined scoring formula weighted by user-defined coefficients like α. This provides transparency in model selection and aligns model behavior with scientific intent.\n",
    "\n",
    "### Follow-up 1.5: Does URF learn anything meaningful about real data, or just statistical artifacts?\n",
    "Most likely both — but importantly, URFs respond to statistical patterns that correlate with real astrophysical behavior, such as dips and variability. This is supported by the high TOI recall and importance scores observed across sectors, particularly for alpha-tuned variants. CLARA Part 2 will further test this by correlating anomalies with physical properties.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2: What hyperparameters were fixed vs randomized across URF-4 subvariants?\n",
    "URF-4 subvariants used a fixed hyperparameter search space inspired by Crake & MG (2023). For each synthetic set design, we applied a random search using the following distributions:\n",
    "\n",
    "`n_estimators`: 10 evenly spaced values between 50 and 200\n",
    "\n",
    "`max_features`: [sqrt, log2]\n",
    "\n",
    "`max_depth`: [100, 300, 500, 700, 900, 1000, None]\n",
    "\n",
    "`min_samples_split`: [2, 4, 7, 10]\n",
    "\n",
    "`min_samples_leaf`: [1, 2]\n",
    "\n",
    "`bootstrap`: [True, False]\n",
    "\n",
    "`warm_start`: [True, False]\n",
    "\n",
    "### Follow-up 2.1: What hyperparameters were fixed for all models?\n",
    "Parameters not included in the search space were fixed for all subvariants:\n",
    "criterion = 'gini', random_state = 42, oob_score = False, class_weight = None, ccp_alpha = 0.0, max_samples = None, monotonic_cst = None, min_impurity_decrease = 0.0.\n",
    "\n",
    "### Follow-up 2.2: Why was this specific search space used?\n",
    "The configuration was directly inspired by Crake & MG (2023), balancing model diversity with computational efficiency. It covers a reasonable range of decision tree depths, ensemble sizes, and split strategies.\n",
    "\n",
    "### Follow-up 2.3: Was hyperparameter tuning linked to performance?\n",
    "Yes. After random search, each URF model was evaluated on a labelled validation set of 4000 light curves using metrics like TOI recall and importance. A combined scoring formula with varying α was used to identify top-performing models under different metric priorities.\n",
    "\n",
    "### Follow-up 2.4: Were any models found to have identical hyperparameters?\n",
    "Yes. Out of 36 URF-4 models, 32 shared hyperparameter configurations in repeated clusters. This indicates that different synthetic feature sets can result in the same optimal hyperparameters.\n",
    "\n",
    "### Follow-up 2.5: Was the hyperparameter search space fixed for every synthetic set?\n",
    "Yes. The same hyperparameter distribution was used for all synthetic set configurations to ensure fair comparison of resulting models.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3: Is there any risk of overfitting to the synthetic contrast class?\n",
    "Yes — as shown by URF-3, which used synthetic curves derived from real TOI features. This model heavily overfit and performed poorly outside its sector. We therefore returned to uniform synthetic distributions as used in Crake & MG (2023), but explored a range of values for features like noise and cadence.\n",
    "\n",
    "### Follow-up 3.1: Did URF-4 overfit in any observable way?\n",
    "No significant overfitting was observed for URF-4 variants. This is likely because the synthetic data remained uniformly distributed and did not mimic TOI structures.\n",
    "\n",
    "### Follow-up 3.2: Can fine-grained tuning of synthetic features help generalization?\n",
    "Yes — but only within viable ranges. For example, noise_ppm between 50 and 300 produced stable models. Outside these ranges, URF behavior became unstable or collapsed entirely.\n",
    "\n",
    "### Follow-up 3.3: Do anomaly scores show selective learning of TOIs?\n",
    "Yes. Despite a low proportion of TOIs per sector (~200 out of 16,000), URF-4 models achieved TOI recall rates comparable to or exceeding their anomaly rate, suggesting some degree of meaningful signal extraction rather than random flagging.\n",
    "\n",
    "### Follow-up 3.4: Could real, labelled negative data improve training?\n",
    "Unlikely. We tested this in URF-3 using confirmed TOIs, and the model overfit. Without a uniform and representative real-negative dataset, URFs generalize poorly when trained on real data distributions.\n",
    "\n",
    "### Follow-up 3.5: Are anomaly scores tied to astrophysical structure?\n",
    "That’s the working hypothesis. While URFs respond to statistical noise, the boundaries they learn appear aligned with real structure (e.g. transits, eclipses). CLARA Part 2 will evaluate this by testing anomaly score correlations with astrophysical properties.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4: Why use terminal node population as the scoring heuristic?\n",
    "This follows the methodology of Baron & Poznanski (2017) and MG21. The URF assigns anomaly scores by:\n",
    "\n",
    "Training a classifier to distinguish real from synthetic curves\n",
    "\n",
    "For each tree, recording how many real curves land in each terminal node\n",
    "\n",
    "Computing a similarity score (S) for each real curve as the average fraction of real data in the same leaf\n",
    "\n",
    "Defining anomaly score = 1 − S\n",
    "\n",
    "A score of 1 means the object is always alone in its terminal node (maximally anomalous), and 0 means it’s always grouped with all others.\n",
    "\n",
    "### Follow-up 4.1: Why choose this scoring method?\n",
    "It provides a principled and scalable way to quantify “anomalousness” based purely on how the forest partitions the feature space. It’s consistent across models and interpretable.\n",
    "\n",
    "### Follow-up 4.2: Is performance stable across sectors?\n",
    "Yes — as long as the synthetic set remains in the stable regime. Alpha-variant behavior for each metric is reproducible across sectors.\n",
    "\n",
    "### Follow-up 4.3: Does the method have scientific value?\n",
    "Yes — while the scoring method is statistical, it produces scientifically meaningful separation across metrics like TOI recall and importance, which hold consistently across sectors.\n",
    "\n",
    "### Follow-up 4.4: Can scores be reproduced?\n",
    "Yes — using the same seed (random_state = 42) and same hyperparameter configuration, anomaly scores are reproducible.\n",
    "\n",
    "### Follow-up 4.5: Can anomaly scores be tied to anomaly types?\n",
    "Not yet — but this is a core goal of CLARA Part 2, which aims to correlate score distributions with specific astrophysical or astrometric properties.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 5: Does the URF learn meaningful separation from synthetic to real data, or just \"weirdness\"?\n",
    "We argue it learns both — the “weirdness” URF responds to often aligns with real astrophysical structure (transits, dips). This is supported by consistent TOI recall and importance across alpha variants.\n",
    "\n",
    "### Follow-up 5.1: Has this been validated?\n",
    "Not fully — but it is planned in Part 2 of CLARA, which will map anomaly scores and clusters to real astrophysical object classes and properties.\n",
    "\n",
    "### Follow-up 5.2: Could URFs be learning pure noise?\n",
    "It’s unlikely. The ratio of TOIs flagged vs total anomalies across sectors suggests some structure is being detected beyond noise.\n",
    "\n",
    "### Follow-up 5.3: Could stronger validation help?\n",
    "Yes — especially if we map scores and anomalies back to physical object classes using Gaia, SIMBAD, or RV surveys. That would offer direct confirmation.\n",
    "\n",
    "### Follow-up 5.4: Does URF respond to signal boundaries?\n",
    "Yes — though it learns statistical thresholds, those appear to correspond to real features in the data, based on recall and importance behavior.\n",
    "\n",
    "### Follow-up 5.5: Is it either pure noise detection or structure detection?\n",
    "No — it’s likely a mix. The model reacts to statistical deviation, but that deviation often reflects meaningful, structured signals in the light curves.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1389718-9dc9-4438-bfde-99473ae78fa3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# III. Synthetic Set Design\n",
    "Why inject transits into the synthetic set rather than the real set (like traditional recovery testing)?\n",
    "\n",
    "Why not use completely random synthetic flux curves?\n",
    "\n",
    "How were parameters like duration, S/N, cadence chosen for each variant?\n",
    "\n",
    "Are the synthetic light curves realistic enough?\n",
    "\n",
    "Could injecting into real light curves lead to stronger contrast?\n",
    "\n",
    "What if the synthetic set is too similar to real ones — would it make scoring noisier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4105b1f9-8bec-4fbb-b231-6ccfae70f818",
   "metadata": {},
   "source": [
    "# IV. Evaluation Metrics\n",
    "Why choose TOI Recall AUC and Importance AUC as the combined score axes?\n",
    "\n",
    "Why not include binary recall, PR-AUC, or anomaly precision?\n",
    "\n",
    "Why 20% for “top N% importance” — is that arbitrary?\n",
    "\n",
    "Why percentile ranges for score concentration rather than mean ranks or cumulative coverage?\n",
    "\n",
    "Are scores consistent across random test subsets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d9e25-1b73-4d71-a3c7-4b36b4bdc29e",
   "metadata": {},
   "source": [
    "# V. Pipeline Execution & Runtime\n",
    "How were the light curves processed so quickly — what exactly did you optimize?\n",
    "\n",
    "Why do your models run 15× faster than MG23 despite same feature length?\n",
    "\n",
    "Is the runtime still scalable to larger sets (e.g. FFIs, LSST)?\n",
    "\n",
    "How many cores and threads were used for parallelism?\n",
    "\n",
    "Could results be replicated on other machines?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8226dd-f9d4-4838-a440-24d7adeca91b",
   "metadata": {},
   "source": [
    "# VI. Alpha Variants & Steering Behavior\n",
    "Why use a combined score to rank model variants — does it reflect real scientific tradeoffs?\n",
    "\n",
    "Why were only α = 0.3, 0.5, and 0.9 tested?\n",
    "\n",
    "How does behavior change across the entire α range?\n",
    "\n",
    "Do lower α models always concentrate TOIs more tightly?\n",
    "\n",
    "Is this behavioral tuning stable across sectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00efda2e-7910-4bc8-aa2b-adaabff73491",
   "metadata": {},
   "source": [
    "# VII. Limitations & Boundaries\n",
    "What does this pipeline not do?\n",
    "\n",
    "Can it detect non-transit anomalies like flares, binaries, blends, systematic artifacts?\n",
    "\n",
    "Is interpretability generalizable beyond TOIs?\n",
    "\n",
    "What if the synthetic set is poorly designed — does URF behavior become meaningless?\n",
    "\n",
    "How do we distinguish a “high-scoring TOI” from a “high-scoring noise artifact”?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8748ffc-2a1e-413e-b7ed-f448ef42e236",
   "metadata": {},
   "source": [
    "# VIII. Scientific Discovery Potential\n",
    "How many new candidates were found?\n",
    "\n",
    "Were any flagged curves previously known or listed elsewhere (e.g. rejected TOIs)?\n",
    "\n",
    "Can this pipeline prioritize candidates for follow-up?\n",
    "\n",
    "Can you target non-transit science with different synthetic sets?\n",
    "\n",
    "How much does the anomaly score correlate with astrophysical properties?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e2b7b2-68f0-46fa-92ef-e775ed153d4f",
   "metadata": {},
   "source": [
    "# IX. Future Use Cases (for Part 2 Preview)\n",
    "Could this pipeline be applied to Kepler, LSST, ZTF, or Gaia light curves?\n",
    "\n",
    "Can clustering methods be layered on top of the anomaly output?\n",
    "\n",
    "Could a public UI or API allow users to tune α and score curves interactively?\n",
    "\n",
    "What improvements can be made to scoring resolution?\n",
    "\n",
    "Can you auto-learn optimal synthetic parameters via meta-optimization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8097617c-74e7-41b6-8327-48226ebfb4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
