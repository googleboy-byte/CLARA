{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf0dbaad-b5d3-46f0-8191-0beb7f064d05",
   "metadata": {},
   "source": [
    "# I. Dataset & Input Design\n",
    "\n",
    "### Question 1: Why were only SPOC 2-minute cadence light curves used?\n",
    "\n",
    "We used 2-minute cadence SPOC light curves because they provide a manageable dataset size (~15k–20k light curves per sector) with high-quality, preprocessed data and confirmed TOIs. This makes them ideal for experimentation on a modest system (32GB RAM, 4-core i3 CPU), and provides a fair testbed without overwhelming computational resources.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-up 1.1: Why not use more stars or the full million-star dataset?\n",
    "\n",
    "More stars were not required to test our hypothesis. We selected a representative subset with a fair distribution of TOIs, sufficient to evaluate model behavior under resource constraints.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-up 1.2: Why not use FFI (30-minute cadence) light curves?\n",
    "\n",
    "FFI light curves are extremely large in volume, making them impractical for this phase of the project on our available hardware. Our goal was not exhaustive coverage, but controlled hypothesis testing.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-up 1.3: Why did you move from Sector 1 to Sector 2?\n",
    "\n",
    "The first four URF models were trained and tested on Sector 1. However, URF-3, trained using synthetic features generated from TOIs, overfitted to Sector 1 data. To confirm this, we tested it on Sector 2, which revealed generalization failure. Due to space limitations, only one sector could be held at a time. Sector 2 was retained for follow-up experiments and URF-4 development. We plan to revisit cross-sector validation in CLARA Part 3.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-up 1.4: Why were TOIs used at all in an unsupervised setting?\n",
    "\n",
    "Since we work with unlabeled data, we needed a ground-truth proxy to benchmark performance. TOIs represent scientifically validated transit signals and serve as a useful reference to evaluate anomaly detection quality.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-up 1.5: Were TOIs used for training?\n",
    "\n",
    "Yes, in URF-3. The synthetic set for URF-3 was constructed from features derived from known TOIs. However, the model overfit to Sector 1 and failed to generalize to Sector 2. This led to the development of URF-4, which relies on synthetic light curves with user-controlled features.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-up 1.6: Why use synthetic light curves (e.g., with `batman`) in URF-4?\n",
    "\n",
    "URF-4 was built to test synthetic set design systematically. Using tools like `batman`, we can generate synthetic LCs with controlled input parameters — such as transit count, duration, cadence, and noise — making it possible to study how these factors influence URF model behavior.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-up 1.7: Why not use TOIs for synthetic set design?\n",
    "\n",
    "While TOIs are useful for evaluation, they lack standardized, controllable input parameters. Synthetic curves, on the other hand, allow precise tuning of design variables like duration, cadence, and noise. This makes them ideal for constructing interpretable and tunable synthetic training sets for URF models.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2: Could the results change if we use FFI-based light curves?\n",
    "\n",
    "Yes, the results would likely change if FFI-based light curves were used. FFI (Full Frame Image) light curves are not subject to the same target pre-selection as 2-minute cadence light curves, and they represent a broader, less filtered stellar population. As a result, the definition of \"normal\" behavior in light curves would shift, potentially altering the anomaly scoring and the mapping between synthetic set parameters and model performance. While our hypothesis — that synthetic set design can steer model behavior — is expected to hold, the specific mappings of input features (e.g., number of transits, duration) to performance metrics (e.g., TOI recall) would likely differ.\n",
    "\n",
    "\n",
    "### Follow-Up 2.1: Why would FFI-based light curves change the anomaly landscape?\n",
    "\n",
    "2-minute cadence light curves are drawn from pre-selected targets, typically nearby dwarf stars, and tend to exhibit more stable, well-characterized variability. In contrast, FFI data includes a much wider range of stellar types and behaviors, including less stable or more variable stars. This broader variability spectrum would redefine what is considered \"normal,\" leading the URF to assign different anomaly scores. Consequently, the anomaly score thresholds — currently calibrated using TOIs from 2-min data — would not directly apply to FFI data.\n",
    "\n",
    "\n",
    "### Follow-Up 2.2: Would anomaly score thresholds need to change for FFI-based models?\n",
    "\n",
    "Yes. Since FFI data would establish a different baseline for what is \"normal,\" the anomaly score distribution would shift. Thresholds based on 2-min cadence TOIs would no longer be valid for performance evaluation. A new evaluation strategy would be needed, possibly using synthetic injections or another benchmark tailored to the FFI population.\n",
    "\n",
    "\n",
    "### Follow-Up 2.3: If TOIs can't serve as benchmarks for FFI-based URFs, how should model performance be evaluated?\n",
    "\n",
    "When TOIs are insufficient or non-representative (as in FFI studies), we can use synthetic light curves with known injected features (e.g., transits, flares) as a benchmarking strategy. This allows for controlled testing of model sensitivity and precision by comparing detection outcomes against known injected events.\n",
    "\n",
    "\n",
    "### Follow-Up 2.4: Can FFI-based models find anomalies that are missed in 2-min cadence data?\n",
    "\n",
    "This has not yet been tested. However, if the FFI-based population defines a new normal, then some light curves that were considered normal in 2-min cadence analysis might appear anomalous when compared against FFI variability patterns. Once Part 2 clustering is complete, we can examine whether new anomaly types emerge uniquely in FFI-trained models.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3: How representative are the 16k light curves per sector?\n",
    "  \n",
    "The ~16,000 light curves per sector (from SPOC 2-minute cadence targets) may not be fully representative of the entire TESS dataset, particularly compared to FFI-based light curves. However, they are standardized, high-quality, and computationally manageable — making them suitable for hypothesis testing. Their use is also consistent with prior work, such as Crake & Martínez-Galarza (2023), which this study builds upon.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-Up 3.1: In what sense are these 16k curves possibly *not* representative of full TESS sectors?\n",
    "\n",
    "\n",
    "They exclude the vast majority of stars observed in TESS sectors — especially those only available through FFIs. The 2-min targets are typically bright, nearby dwarfs selected for known scientific interest, leading to a biased sample with less astrophysical diversity.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-Up 3.2: How did you account for any imbalance or bias when selecting test subsets from the 16k curves?\n",
    "\n",
    "\n",
    "We used stratified random sampling to match the TOI-to-total curve ratio of the full 16k set. This method was used both for the 4000-light curve test subset (used in URF-4 subvariant evaluation) and the 10 subsets (used in α-variant testing). It ensured uniformity and fair comparison across models.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-Up 3.3: Why is this sample still valid for hypothesis testing?\n",
    "\n",
    "\n",
    "Because the primary goal was to test how synthetic set design affects URF performance in a controlled setting. The relative behavior of models — not the absolute astrophysical coverage — was the focus. This dataset offers clarity, consistency, and efficiency for such controlled evaluations.\n",
    "\n",
    "\n",
    "### Follow-Up 3.4: What limitations does this dataset impose on the generalizability of your conclusions?\n",
    "\n",
    "\n",
    "Being restricted to 2-min cadence targets means the results might not generalize to FFI data, which includes more diverse and noisier light curves. Definitions of normality, anomaly thresholds, and performance metrics like TOI recall or importance AUC may shift significantly with FFI-based studies.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-Up 3.5: How could future CLARA research address the representativeness limitations of the current 16k light curve subset?\n",
    "\n",
    "\n",
    "By scaling testing to FFI-based datasets using high-performance computing. This would allow us to evaluate how the performance-to-input feature mapping changes across a broader population and define correction strategies or new models suited for FFI-normalized anomaly detection.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Question 4: Are TOIs evenly distributed in the set? Are there biases?\n",
    "\n",
    "\n",
    "No, TOIs are not evenly distributed. In Sector 1, there were 175 TOI-associated FITS files out of ~16,000 light curves; in Sector 2, the number was 195. The distribution appears sparse, and there is no evidence of uniform coverage across all TICs in a sector.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-Up 4.1 Have you tried to quantify TOI biases with respect to sky position or magnitude?  \n",
    "No. We have not yet correlated astrometric or astrophysical properties to the TESS data. For URF feature extraction, we used only flux time series and Lomb-Scargle periodogram power values (vector sizes 3000 and 1000 respectively).\n",
    "\n",
    "\n",
    "\n",
    "### Follow-Up 4.2 Would this bias in TOI distribution affect the model’s ability to generalize?  \n",
    "This has not been definitively proven, but we believe not. The bias is consistent (~175–195 TOIs out of 16k light curves per sector). Since only 1500 curves were used as the real feature set during URF training and hyperparameter search, and the URF-4 α = 0.5 model passed the generalization test, the model appears robust to such distributional bias.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-Up 4.3 Do we know what selection effects determine which TICs are assigned TOIs in SPOC data?  \n",
    "No. This hasn't been explored in our work. TOIs are used solely as a proxy benchmark for evaluating anomaly detection, not as an unbiased sample of transit candidates.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-Up 4.4 Do TOIs cluster in anomaly score space, or are they evenly spread?  \n",
    "They tend to cluster near the top of the anomaly score distribution. In our analysis, high-importance TOIs were often found within the top 5–20% of the anomaly score percentiles, especially in balanced and low-α URF-4 variants.\n",
    "\n",
    "\n",
    "\n",
    "### Follow-Up 4.5 Could uneven representation of astrophysical TOI types affect model performance across anomaly categories?  \n",
    "Possibly. Our synthetic sets used fixed parameter configurations and transit models (box or Mandel-Agol). If TOIs skew heavily toward certain types (e.g., deep, short-period hot Jupiters), then less-represented types may be underperforming. However, generalization tests suggest the model handles diverse anomaly shapes reasonably well.\n",
    "\n",
    "---\n",
    "\n",
    "### Question 5: What preprocessing (detrending, smoothing, normalization) was used, and how does it affect anomalies?\n",
    "\n",
    "For each light curve, we generated a 4000-length feature vector:  \n",
    "- The first 3000 values were normalized PDCSAP flux points from the start of the light curve.  \n",
    "- The remaining 1000 values came from the Lomb-Scargle periodogram’s power spectrum.  \n",
    "\n",
    "This setup captures both the direct flux time series and its dominant periodic features — enabling the URF to learn from both temporal behavior and frequency domain patterns. Normalization was critical to ensure comparability between flux values across stars, and truncation was used for computational efficiency.\n",
    "\n",
    "\n",
    "### Follow-Up 5.1: Why 3000 flux points and 1000 power values specifically?\n",
    "  \n",
    "The decision was inspired by Crake & Martínez-Galarza (2023), who also used flux + periodogram features for anomaly detection.  \n",
    "Practically, using all ~19,000 flux points is computationally expensive. We assumed that if any periodic anomaly exists, its signature would appear in the first 3000 points.  \n",
    "This trade-off balances detection capability and computational tractability.\n",
    "\n",
    "\n",
    "### Follow-up 5.2: How was normalization applied, and why is it important?\n",
    " \n",
    "Normalization was done using `.normalize()` from `astropy`, applied after creating the light curve from time and flux data.  \n",
    "It standardizes the scale of flux values across all stars, allowing the URF to compare light curves meaningfully.  \n",
    "Without normalization, variations due to stellar brightness differences could obscure true anomalies.\n",
    "\n",
    "\n",
    "### Follow-up 5.3: Why was truncation to 3000 flux points considered safe?\n",
    "  \n",
    "While full light curves have ~19,000 points, analyzing all is inefficient and often redundant.  \n",
    "Since many periodic anomalies (e.g. transits or flares) repeat or occur early, we assumed the first 3000 points would usually capture relevant behavior.  \n",
    "This assumption held well during URF training and testing phases.\n",
    "\n",
    "\n",
    "### Follow-up 5.4: Why was the Lomb-Scargle periodogram used, and how do its power values help in detecting anomalies?\n",
    "  \n",
    "Lomb-Scargle is ideal for unevenly sampled time series like TESS light curves.  \n",
    "Its power spectrum identifies dominant periodicities, which helps the URF learn what constitutes a \"periodic anomaly.\"  \n",
    "The peak frequency from the power spectrum is later used to phase-fold light curves and study recurring patterns more clearly.\n",
    "\n",
    "\n",
    "### Follow-up 5.5: How might preprocessing choices introduce biases or affect anomaly detection?\n",
    "  \n",
    "Normalization and truncation can inadvertently suppress low-amplitude or long-duration anomalies.  \n",
    "For instance, an overly aggressive normalization may flatten subtle dips, and truncating at 3000 points may miss late-occurring periodic events.  \n",
    "Thus, preprocessing decisions involve trade-offs between speed, sensitivity, and signal completeness.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefa6439-b42e-448e-801e-4a985fcc2ce5",
   "metadata": {},
   "source": [
    "# II. URF Architecture & Model Logic\n",
    "Why URF instead of other anomaly detectors (e.g., autoencoders, isolation forests, VAEs)?\n",
    "\n",
    "What hyperparameters were fixed vs randomized across variants?\n",
    "\n",
    "Is there any risk of overfitting to synthetic contrast class?\n",
    "\n",
    "Why use terminal node population as the scoring heuristic?\n",
    "\n",
    "Does the URF learn meaningful separation from synthetic to real data, or just \"weirdness\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1389718-9dc9-4438-bfde-99473ae78fa3",
   "metadata": {},
   "source": [
    "# III. Synthetic Set Design\n",
    "Why inject transits into the synthetic set rather than the real set (like traditional recovery testing)?\n",
    "\n",
    "Why not use completely random synthetic flux curves?\n",
    "\n",
    "How were parameters like duration, S/N, cadence chosen for each variant?\n",
    "\n",
    "Are the synthetic light curves realistic enough?\n",
    "\n",
    "Could injecting into real light curves lead to stronger contrast?\n",
    "\n",
    "What if the synthetic set is too similar to real ones — would it make scoring noisier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4105b1f9-8bec-4fbb-b231-6ccfae70f818",
   "metadata": {},
   "source": [
    "# IV. Evaluation Metrics\n",
    "Why choose TOI Recall AUC and Importance AUC as the combined score axes?\n",
    "\n",
    "Why not include binary recall, PR-AUC, or anomaly precision?\n",
    "\n",
    "Why 20% for “top N% importance” — is that arbitrary?\n",
    "\n",
    "Why percentile ranges for score concentration rather than mean ranks or cumulative coverage?\n",
    "\n",
    "Are scores consistent across random test subsets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d9e25-1b73-4d71-a3c7-4b36b4bdc29e",
   "metadata": {},
   "source": [
    "# V. Pipeline Execution & Runtime\n",
    "How were the light curves processed so quickly — what exactly did you optimize?\n",
    "\n",
    "Why do your models run 15× faster than MG23 despite same feature length?\n",
    "\n",
    "Is the runtime still scalable to larger sets (e.g. FFIs, LSST)?\n",
    "\n",
    "How many cores and threads were used for parallelism?\n",
    "\n",
    "Could results be replicated on other machines?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8226dd-f9d4-4838-a440-24d7adeca91b",
   "metadata": {},
   "source": [
    "# VI. Alpha Variants & Steering Behavior\n",
    "Why use a combined score to rank model variants — does it reflect real scientific tradeoffs?\n",
    "\n",
    "Why were only α = 0.3, 0.5, and 0.9 tested?\n",
    "\n",
    "How does behavior change across the entire α range?\n",
    "\n",
    "Do lower α models always concentrate TOIs more tightly?\n",
    "\n",
    "Is this behavioral tuning stable across sectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00efda2e-7910-4bc8-aa2b-adaabff73491",
   "metadata": {},
   "source": [
    "# VII. Limitations & Boundaries\n",
    "What does this pipeline not do?\n",
    "\n",
    "Can it detect non-transit anomalies like flares, binaries, blends, systematic artifacts?\n",
    "\n",
    "Is interpretability generalizable beyond TOIs?\n",
    "\n",
    "What if the synthetic set is poorly designed — does URF behavior become meaningless?\n",
    "\n",
    "How do we distinguish a “high-scoring TOI” from a “high-scoring noise artifact”?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8748ffc-2a1e-413e-b7ed-f448ef42e236",
   "metadata": {},
   "source": [
    "# VIII. Scientific Discovery Potential\n",
    "How many new candidates were found?\n",
    "\n",
    "Were any flagged curves previously known or listed elsewhere (e.g. rejected TOIs)?\n",
    "\n",
    "Can this pipeline prioritize candidates for follow-up?\n",
    "\n",
    "Can you target non-transit science with different synthetic sets?\n",
    "\n",
    "How much does the anomaly score correlate with astrophysical properties?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e2b7b2-68f0-46fa-92ef-e775ed153d4f",
   "metadata": {},
   "source": [
    "# IX. Future Use Cases (for Part 2 Preview)\n",
    "Could this pipeline be applied to Kepler, LSST, ZTF, or Gaia light curves?\n",
    "\n",
    "Can clustering methods be layered on top of the anomaly output?\n",
    "\n",
    "Could a public UI or API allow users to tune α and score curves interactively?\n",
    "\n",
    "What improvements can be made to scoring resolution?\n",
    "\n",
    "Can you auto-learn optimal synthetic parameters via meta-optimization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8097617c-74e7-41b6-8327-48226ebfb4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
